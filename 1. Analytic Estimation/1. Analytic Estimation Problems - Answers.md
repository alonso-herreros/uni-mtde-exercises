## Modern Theory of Detection and Estimation

# Analytic Estimation Problems - Answers

*Academic year 2024-2025*  

$$
\global\def\E#1{\mathbb E \left\{#1\right\}}
\global\def\Var#1{\mathrm{Var} \left\{#1\right\}}
\global\def\Cov{\mathrm{Cov}}
\global\def\est#1#2{\hat{#1}_{\text{#2}}}
\global\def\ML#1{\est{#1}{ML}}
\global\def\MAP#1{\est{#1}{MAP}}
\global\def\MSE#1{\est{#1}{MSE}}
\global\def\MMSE#1{\est{#1}{MMSE}}
$$

---

## Exercise 4 (1.5)

### Question a) Find $\hat{S}_1$ and $\hat{S}_2$

#### 1. Finding $\hat{S}_1$

We are tasked with finding an expression for the estimator of $S$ given $X_1$, $\hat{S}_1$.

This can be found using the known formula:

$$
\hat{S}_1 = m_S + \frac{v_{SX_1}}{v_{X_1}} (X_1 - m_{X_1})
$$

We just need to find the value of the parameters.

$$
\begin{aligned}
    m_S &= 0 \phantom{\underset{0}{\normal0}}\\
    m_{X_1} &= \E{S + N_1} \\
        &= \E{S} + \E{N_1} \\
        &= 0 \phantom{\underset{0}{\normal0}}\\
    v_{SX_1} &= \E{SX_1} - \E{S}\E{X_1} \\
        &= \E{S (S+N_1)} - 0 \\
        &= \E{S^2 + SN_1} \\
        &= \E{S^2} + \E{SN_1} \\
        &= \Var{S} + \E{S}^2 + \Cov(S, N_1) + \E{S}\E{N_1} \\
        &= v_S \phantom{\underset{0}{\normal0}}\\
    v_{X_1} &= \E{X_1^2} - \E{X_1}^2 \\
        &= \E{(S + N_1)^2} - \E{S + N_1}^2 \\
        &= \E{S^2 + 2SN_1 + N_1^2} - \left(\E{S} + \E{N_1}\right)^2 \\
        &= \E{S^2} + 2\E{SN_1} + \E{N_1^2} - 0 \\
        &= \Var{S} + \E{S}^2 +  \Var{N_1} + \E{N_1}^2 + 2\E{S}\E{N_1} \\
        &= v_S + v_n
\end{aligned}
$$

Substituting these values into the formula for $\hat{S}_1$:

$$
\begin{aligned}
    \hat{S}_1 &= 0 + \frac{v_S}{v_S + v_n} (X_1 - 0) \\
    &\boxed{= \frac{v_S}{v_S + v_n} X_1}
\end{aligned}
$$

#### 2. Finding $\hat{S}_2$

This will be similar to the previous part, but using $X_2$ instead of $X_1$.

$$
\hat{S}_2 = m_S + \frac{v_{SX_2}}{v_{X_2}} (X_2 - m_{X_2})
$$

Next, to find the values of the parameters

$$
\begin{aligned}
    m_S &= 0 \phantom{\underset{0}{\normal0}}\\
    m_{X_2} &= \E{S + N_2} \\
        &= \E{S} + \E{N_2} \\
        &= 0 \phantom{\underset{0}{\normal0}}\\
    v_{SX_2} &= \E{SX_2} - \E{S}\E{X_2} \\
        &= \E{S (αS+N_2)} - \E{S} \E{αS+N_2} \\
        &= \E{αS^2 + SN_2} - 0\\
        &= α\E{S^2} + \E{SN_2} \\
        &= α\left(\Var{S} + \E{S}^2\right) + \Cov(S, N_2) + \E{S}\E{N_2} \\
        &= αv_S \phantom{\underset{0}{\normal0}}\\
    v_{X_2} &= \E{X_2^2} - \E{X_2}^2 \\
        &= \E{(αS + N_2)^2} - \E{αS + N_2}^2 \\
        &= \E{α^2S^2 + 2αSN_2 + N_2^2} - \left(\E{αS} + \E{N_2}\right)^2 \\
        &= α^2\E{S^2} + 2α\E{SN_2} + \E{N_2^2} - 0 \\
        &= α^2\left(\Var{S} + \E{S}^2\right) + \Var{N_2} + \E{N_2}^2 + 2\E{S}\E{N_2} \\
        &= α^2 v_S + v_n
\end{aligned}
$$

With these values, we can substitute them into the formula for $\hat{S}_2$:

$$
\begin{aligned}
    \hat{S}_2 &= 0 + \frac{αv_S}{α^2 v_S + v_n} (X_2 - 0) \\
    &\boxed{= \frac{αv_S}{α^2 v_S + v_n} X_2}
\end{aligned}
$$

### Question b) Find MSE of each estimator and compare for different values of α

The mean square error is defined as:

$$
\begin{aligned}
    \E{(S-\hat{S})^2}
\end{aligned}
$$

#### 1. Finding $\E{(S-\hat{S}_1)^2}$

$$
\begin{aligned}
    \E{(S-\hat{S}_1)^2} &= \E{\left(S - \frac{v_S}{v_S + v_n} X_1\right)^2} \\
    &= \E{S^2} - 2\frac{v_S}{v_S+v_n} \E{S X_1} + \frac{v_S^2}{(v_S+v_n)^2} \E{X_1^2} \\
    &= \Var{S} + \frac{v_S^2}{(v_S+v_n)^2} \E{(S + N_1)^2}
        + 2 \frac{v_S}{v_S+v_n} \E{S(S+N_1)} \\
    &= v_S + \frac{v_S^2}{(v_S+v_n)^2} \left(\E{S^2} + \E{N_1^2} + 2\E{SN_1}\right)
        - 2 \frac{v_S}{v_S+v_n} \left(\E{S^2} + \E{S N_1}\right) \\
    &= v_S + \frac{v_S^2}{(v_S+v_n)^2} (\Var{S} + \Var{N_1})
        - 2 \frac{v_S}{v_S+v_n} (\Var{S}) \\
    &= v_S + \frac{v_S^2}{v_S+v_n} - 2 \frac{v_S^2}{v_S+v_n} \\
    &= \frac{v_S (v_S + v_n)}{v_S + v_n} - \frac{v_S^2}{v_S+v_n} \\
    &= \frac{v_S ⋅ v_n}{v_S + v_n}
\end{aligned}
$$

#### 2. Finding $\E{(S-\hat{S}_2)^2}$

$$
\begin{aligned}
    \E{(S-\hat{S}_2)^2} &=&& \E{\left(S - \frac{αv_S}{α^2 v_S + v_n} X_2\right)^2} \\
    &=&& \E{S^2} + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2} \E{X_2^2}
        - 2 \frac{αv_S}{α^2 v_S+v_n} \E{SX_2} \\
    &=&& \Var{S} + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2} \E{(αS + N_2)^2}
        - \frac{2 αv_S}{α^2 v_S+v_n} \E{S(αS+N_2)} \\
    &=&& v_S + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2} \left(α^2\E{S^2} + \E{N_2^2} +2α\E{SN_2}\right) \\
      &&& - \frac{2 αv_S}{α^2 v_S+v_n} \left(α\E{S^2} + \E{S N_2}\right) \\
    &=&& v_S + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2}\left(α^2v_S + v_n\right)
        - \frac{2 αv_S}{α^2 v_S+v_n} αv_S \\
    &=&& \frac{v_S (α^2v_S + v_n)}{α^2 v_S+v_n} + \frac{α^2v_S^2}{α^2 v_S+v_n}
        - 2 \frac{α^2v_S^2}{α^2 v_S+v_n} \\
    &=&& \frac{v_S ⋅ v_n}{α^2 v_S+v_n}
\end{aligned}
$$

#### 3. Comparing both

$$
\begin{aligned}
&& \E{(S-\hat{S}_1)^2} &> \E{(S-\hat{S}_2)^2} &⟺ \\
&⟺& \frac{v_S ⋅ v_n}{v_S + v_n} &> \frac{v_S ⋅ v_n}{α^2 v_S+v_n} &⟺\\
&⟺& α^2 v_S+v_n &> v_S + v_n &⟺\\
&⟺& |α| &> 1
\end{aligned}
$$

#### Results

$$
\begin{aligned}
    & \boxed{\E{(S-\hat{S}_1)^2} = \frac{v_S ⋅ v_n}{v_S + v_n}} \\
    & \boxed{\E{(S-\hat{S}_2)^2} = \frac{v_S ⋅ v_n}{α^2 v_S+v_n}} \\
    & \boxed{\E{(S-\hat{S}_1)^2} > \E{(S-\hat{S}_2)^2} ⟺ |α| > 1}
\end{aligned}
$$

### Question c) Find $\hat{S}_{MMSE}(\bar{X})$

Since the variables are linear and independent, their joint pdfs are also gaussian, so the MMSE
estimator is linear and can be expressed as

$$
\hat{S}_{MMSE}(\bar{X}) = \bar{w}_0 + w^T \bar{X}
$$

Substituting each parameter by its respective formula (as solved in the notes)

$$
\begin{aligned}
    \hat{S}_{MMSE}(\bar{X}) &= \E{S} - \bar{c}_{\bar{X},S}^T C_{\bar{X},\bar{X}}^{-1} \E{\bar{X}}
        + \left(C_{\bar{X}, \bar{X}}^{-1} c_{\bar{X}, S}\right)^T \bar{X} \\
    &= \E{S} + \bar{c}_{\bar{X},S}^T C_{\bar{X},\bar{X}}^{-1} \left(\bar{X} - \E{\bar{X}}\right) \\
    &= 0 + \begin{bmatrix}
            v_{SX_1} \\
            v_{SX_2}
        \end{bmatrix}^T \begin{bmatrix}
            v_{X_1} & v_{X_1X_2} \\
            v_{X_2 X_1} & v_{X_2}
        \end{bmatrix}^{-1} \begin{bmatrix}
            \left(X_1 - \E{X_1}\right) \\
            \left(X_2 - \E{X_2}\right)
        \end{bmatrix}
\end{aligned}
$$

Of those values, we're missing $v_{X_1X_2}$ and $v_{X_2X_1}$. Of course, they are equal.

$$
\begin{aligned}
    v_{X_1X_2} &= \E{X_1 X_2} - \E{X_1} \E{X_2} \\
    &= \E{(S + N_1)(αS + N_2)} - \E{S + N_1} \E{αS + N_2} \\
    &= \E{αS^2 + SN_2 + αSN_1 + N_1 N_2} - (0+0) (0+0) \\
    &= α\E{S^2} + \E{SN_2} + α\E{SN_1} + \E{N_1 N_2} \\
    &= α\Var{S} + \Cov(S,N_2) + α\Cov(S,N_1) + \Cov(N_1, N_2) \\
    &= αv_S
\end{aligned}
$$

Then, we can substitute

$$
\begin{aligned}
    \hat{S}_{MMSE} &= \begin{bmatrix}
            v_S \\
            α v_S
        \end{bmatrix}^T \begin{bmatrix}
            v_S + v_n & α v_S \\
            α v_S     & α^2 v_S + v_n
        \end{bmatrix}^{-1} \begin{bmatrix}
            X_1 \\
            X_2
        \end{bmatrix} \\

    &= \begin{bmatrix}
            v_S \\
            α v_S
        \end{bmatrix}^T
        \frac{1}{\left(v_S+v_n\right)\left(α^2v_S+v_n\right) - α^2v_S^2}
        \begin{bmatrix}
            α^2 v_S + v_n & -αv_S \\
            -αv_S         & v_S + v_n
        \end{bmatrix}
        \begin{bmatrix}
            X_1 \\
            X_2
        \end{bmatrix} \\
    &= \frac{1}{α^2v_S^2 + v_S v_n + α^2 v_S v_n + v_n^2 - α^2 v_S^2}
        \begin{bmatrix}
            v_S (α^2 v_S + v_n) - α^2 v_S^2 \\
            - α v_S^2 + α v_S (v_S + v_n)
        \end{bmatrix}
        \begin{bmatrix}
            X_1 \\
            X_2
        \end{bmatrix} \\
    &= \frac{1}{(1+α^2) v_S v_n + v_n^2} \left(v_S v_n X_1 + αv_s v_n X_2\right) \\
    &= \boxed{\frac{1}{1+α^2 +\frac{v_n}{v_S}} X_1 + \frac{α X_2}{1+α^2 +\frac{v_n}{v_S}} X_2}
\end{aligned}
$$

## Exercise 6 (1.2; 1.3; 1.4; 1.7)

### Question a) Find $p_{X|S}(x|s)$

Let's use the properties of variable transformations. First, we'll need to find the expression of
$R$ in terms of $X$ and $S$.

$$
R = X ⋅ S
$$

Now, we'll apply the formula for the PDF of a transformed variable:

$$
\begin{aligned}
    p_{X|S}(x|s) &= p_{R|S} (r = x⋅s) \left|\frac{∂}{∂x} x⋅s\right| \\
    &= \exp (-x⋅s)⋅|s|, \quad x⋅s > 0 \\
    &= \boxed{s⋅\exp (-x⋅s), \quad x > 0}
\end{aligned}
$$

### Question b) Find $\ML{S}$

$$
\ML{S} = \arg \max_s p_{X|S}(x|s)
$$

To find this,

$$
\begin{aligned}
    \frac{∂}{∂s} p_{X|S}(x|s) \Bigg|_{S=\ML{S}} &= 0 \\
    \exp(-x⋅\ML{s}) + -x\ML{s} ⋅ \exp(-x⋅\ML{s}) &= 0 \\
    (1 - x\ML{s}) \exp(-x⋅\ML{s}) &= 0 \\
    1 - x\ML{s} &= 0 \\
    \boxed{\ML{s} = \frac{1}{x}}
\end{aligned}
$$

### Question c) Find $p_{S,X}(s,x)$ and $p_{S|X}(s|x)$

$$
\begin{aligned}
    p_{S,X}(s,x) &= p_{X|S}(x|s) p_S(s) \\
    &= s \exp(-x⋅s) \exp(-s), \quad x > 0\\
    &= \boxed{s \exp(-s(1+x)), \quad x > 0}
\end{aligned}
$$

Now, for the *a posteriori* distribution:

$$
p_{S|X}(s|x) = \frac{p_{S,X}(s,x)}{p_X(x)}
$$

First, we'll have to find $p_X(x)$. It can be found as the marginal of $p_{S,X}(s,x)$:

$$
\begin{aligned}
    p_X(x) &= ∫_{(S)} p_{S,X}(s,x) ds \\
    &= ∫_0^∞ s \exp(-s(1+x)) ds, \quad x>0\\
    &= \left.\frac{-s}{x+1} \exp(-s(x+1))\right|_{s=0}^∞ + \frac{1}{x+1} ∫_0^∞ \exp(-s(x+1)) ds \\
    &= \frac{-1}{(x+1)^2}\exp(-s(x+1))\Big|_{s=0}^∞ \\
    &= \frac{1}{(x+1)^2}, \quad x>0
\end{aligned}
$$

And then,

$$
\boxed{p_{S|X}(s|x) = (x+1)^2 s \exp(-s(1+x)), \quad x>0}
$$

### Question d) Find $\MAP{S}(x)$

$$
\MAP{S}(x) = \arg \max_s p_{S|X}(s|x)
$$

To find this,

$$
\begin{aligned}
    \frac{∂}{∂s} p_{S|X} (s|x) \Bigg|_{s=\MAP{s}} &= 0 \\
    (x+1)^2 \Big(\exp(-s(x+1)) - s(x+1) \exp(-s(x+1))\Big) &= 0 \\
    1 - s(x+1) &= 0 \\
    \boxed{\MAP{s} = \frac{1}{x+1}}
\end{aligned}
$$

### Question e) Find $\MSE{S}(x)$

$$
\begin{aligned}
    \MSE{s}(x) &=&& \E{S|X=x} \\
    &=&& ∫_{(S)} s p_{S|X}(s|x) ds \\
    &=&& ∫_0^∞ s (x+1)^2 s \exp(-s(1+x)) ds, \quad x>0 \\
    &=&& (x+1)^2 \left.\frac{-s^2}{(x+1)} \exp(-s(x+1))\right|_{s=0}^∞ \\
        &&& + \frac{1}{x+1} ∫_0^∞ 2s \exp(-s(x+1)) ds \\
    &=&& 0 + 2 (x+1) \left. \frac{-s}{x+1} \exp(-s(x+1)) \right|_{s=0}^∞ \\
        &&& + \frac{1}{x+1} ∫_0^∞ \exp(-s(x+1)) ds \\
    &=&& - \frac{2}{x+1} \exp(-s(x+1))\Big|_{s=0}^∞ \\
    &=&& \boxed{\frac{2}{x+1}}
\end{aligned}
$$

### Question f) Find the bias of $\MAP{S}$ and $\MSE{S}$

#### 1. Bias of $\MAP{S}$

$$
\begin{aligned}
    \E{S-\MAP{S}} &= \E{S -\frac{1}{x+1}} \\
    &= \E{S} - \E{\frac{1}{x+1}} \\
\end{aligned}
$$

We'll find each of those expected values separately.

$$
\begin{aligned}
    \E{S} &= ∫_{(S)} s p_S(s) ds \\
    &= ∫_0^∞ s \exp(-s) ds \\
    &= \left. -s \exp(-s) \right|_{s=0}^∞ + ∫_0^∞ \exp(-s) ds \\
    &= 0 - \exp(-s) \Big|_{s=0}^∞ \\
    &= +1
\end{aligned}
$$

$$
\begin{aligned}
    \E{\frac{1}{x+1}} &= ∫_{(X)} \frac{1}{x+1} p_X(x) dx \\
    &= ∫_0^∞ \frac{1}{(x+1)^3} dx \\
    &= \frac{(x+1)^{-2}}{-2} \Big|_{x=0}^∞ \\
    &= \frac{1}{2}
\end{aligned}
$$

Substituting these values back into the original equation:

$$
\begin{aligned}
    \E{S-\MAP{S}} &= 1 - \frac{1}{2} \\
    &= \boxed{\frac{1}{2}}
\end{aligned}
$$

#### 2. Bias of $\MSE{S}$

$$
\begin{aligned}
    \E{S-\MSE{S}} &= \E{S - \frac{2}{x+1}} \\
    &= \E{S} - 2\E{\frac{1}{x+1}} \\
    &= 1 - 2 \frac{1}{2} \\
    &= \boxed{0}
\end{aligned}
$$

## Modern Theory of Detection and Estimation

# Analytic Estimation Problems - Answers

*Academic year 2024-2025*  

$$
\global\def\E#1{\mathbb E \left\{#1\right\}}
\global\def\Var#1{\mathrm{Var} \left\{#1\right\}}
\global\def\Cov{\mathrm{Cov}}
\global\def\sgn{\mathrm{sgn}}
\global\def\est#1#2{\hat{#1}_{\text{#2}}}
\global\def\ML#1{\est{#1}{ML}}
\global\def\MAP#1{\est{#1}{MAP}}
\global\def\MAD#1{\est{#1}{MAD}}
\global\def\MSE#1{\est{#1}{MSE}}
\global\def\MMSE#1{\est{#1}{MMSE}}
$$

---

## Exercise 4 (1.5)

### Question a) Find $\hat{S}_1$ and $\hat{S}_2$

#### 1. Finding $\hat{S}_1$

We are tasked with finding an expression for the estimator of $S$ given $X_1$, $\hat{S}_1$.

This can be found using the known formula:

$$
\hat{S}_1 = m_S + \frac{v_{SX_1}}{v_{X_1}} (X_1 - m_{X_1})
$$

We just need to find the value of the parameters.

$$
\begin{aligned}
    m_S &= 0 \phantom{\underset{0}{\normal0}}\\
    m_{X_1} &= \E{S + N_1} \\
        &= \E{S} + \E{N_1} \\
        &= 0 \phantom{\underset{0}{\normal0}}\\
    v_{SX_1} &= \E{SX_1} - \E{S}\E{X_1} \\
        &= \E{S (S+N_1)} - 0 \\
        &= \E{S^2 + SN_1} \\
        &= \E{S^2} + \E{SN_1} \\
        &= \Var{S} + \E{S}^2 + \Cov(S, N_1) + \E{S}\E{N_1} \\
        &= v_S \phantom{\underset{0}{\normal0}}\\
    v_{X_1} &= \E{X_1^2} - \E{X_1}^2 \\
        &= \E{(S + N_1)^2} - \E{S + N_1}^2 \\
        &= \E{S^2 + 2SN_1 + N_1^2} - \left(\E{S} + \E{N_1}\right)^2 \\
        &= \E{S^2} + 2\E{SN_1} + \E{N_1^2} - 0 \\
        &= \Var{S} + \E{S}^2 +  \Var{N_1} + \E{N_1}^2 + 2\E{S}\E{N_1} \\
        &= v_S + v_n
\end{aligned}
$$

Substituting these values into the formula for $\hat{S}_1$:

$$
\begin{aligned}
    \hat{S}_1 &= 0 + \frac{v_S}{v_S + v_n} (X_1 - 0) \\
    &\boxed{= \frac{v_S}{v_S + v_n} X_1}
\end{aligned}
$$

#### 2. Finding $\hat{S}_2$

This will be similar to the previous part, but using $X_2$ instead of $X_1$.

$$
\hat{S}_2 = m_S + \frac{v_{SX_2}}{v_{X_2}} (X_2 - m_{X_2})
$$

Next, to find the values of the parameters

$$
\begin{aligned}
    m_S &= 0 \phantom{\underset{0}{\normal0}}\\
    m_{X_2} &= \E{S + N_2} \\
        &= \E{S} + \E{N_2} \\
        &= 0 \phantom{\underset{0}{\normal0}}\\
    v_{SX_2} &= \E{SX_2} - \E{S}\E{X_2} \\
        &= \E{S (αS+N_2)} - \E{S} \E{αS+N_2} \\
        &= \E{αS^2 + SN_2} - 0\\
        &= α\E{S^2} + \E{SN_2} \\
        &= α\left(\Var{S} + \E{S}^2\right) + \Cov(S, N_2) + \E{S}\E{N_2} \\
        &= αv_S \phantom{\underset{0}{\normal0}}\\
    v_{X_2} &= \E{X_2^2} - \E{X_2}^2 \\
        &= \E{(αS + N_2)^2} - \E{αS + N_2}^2 \\
        &= \E{α^2S^2 + 2αSN_2 + N_2^2} - \left(\E{αS} + \E{N_2}\right)^2 \\
        &= α^2\E{S^2} + 2α\E{SN_2} + \E{N_2^2} - 0 \\
        &= α^2\left(\Var{S} + \E{S}^2\right) + \Var{N_2} + \E{N_2}^2 + 2\E{S}\E{N_2} \\
        &= α^2 v_S + v_n
\end{aligned}
$$

With these values, we can substitute them into the formula for $\hat{S}_2$:

$$
\begin{aligned}
    \hat{S}_2 &= 0 + \frac{αv_S}{α^2 v_S + v_n} (X_2 - 0) \\
    &\boxed{= \frac{αv_S}{α^2 v_S + v_n} X_2}
\end{aligned}
$$

### Question b) Find MSE of each estimator and compare for different values of α

The mean square error is defined as:

$$
\begin{aligned}
    \E{(S-\hat{S})^2}
\end{aligned}
$$

#### 1. Finding $\E{(S-\hat{S}_1)^2}$

$$
\begin{aligned}
    \E{(S-\hat{S}_1)^2} &= \E{\left(S - \frac{v_S}{v_S + v_n} X_1\right)^2} \\
    &= \E{S^2} - 2\frac{v_S}{v_S+v_n} \E{S X_1} + \frac{v_S^2}{(v_S+v_n)^2} \E{X_1^2} \\
    &= \Var{S} + \frac{v_S^2}{(v_S+v_n)^2} \E{(S + N_1)^2}
        + 2 \frac{v_S}{v_S+v_n} \E{S(S+N_1)} \\
    &= v_S + \frac{v_S^2}{(v_S+v_n)^2} \left(\E{S^2} + \E{N_1^2} + 2\E{SN_1}\right)
        - 2 \frac{v_S}{v_S+v_n} \left(\E{S^2} + \E{S N_1}\right) \\
    &= v_S + \frac{v_S^2}{(v_S+v_n)^2} (\Var{S} + \Var{N_1})
        - 2 \frac{v_S}{v_S+v_n} (\Var{S}) \\
    &= v_S + \frac{v_S^2}{v_S+v_n} - 2 \frac{v_S^2}{v_S+v_n} \\
    &= \frac{v_S (v_S + v_n)}{v_S + v_n} - \frac{v_S^2}{v_S+v_n} \\
    &= \frac{v_S ⋅ v_n}{v_S + v_n}
\end{aligned}
$$

#### 2. Finding $\E{(S-\hat{S}_2)^2}$

$$
\begin{aligned}
    \E{(S-\hat{S}_2)^2} &=&& \E{\left(S - \frac{αv_S}{α^2 v_S + v_n} X_2\right)^2} \\
    &=&& \E{S^2} + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2} \E{X_2^2}
        - 2 \frac{αv_S}{α^2 v_S+v_n} \E{SX_2} \\
    &=&& \Var{S} + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2} \E{(αS + N_2)^2}
        - \frac{2 αv_S}{α^2 v_S+v_n} \E{S(αS+N_2)} \\
    &=&& v_S + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2} \left(α^2\E{S^2} + \E{N_2^2} +2α\E{SN_2}\right) \\
      &&& - \frac{2 αv_S}{α^2 v_S+v_n} \left(α\E{S^2} + \E{S N_2}\right) \\
    &=&& v_S + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2}\left(α^2v_S + v_n\right)
        - \frac{2 αv_S}{α^2 v_S+v_n} αv_S \\
    &=&& \frac{v_S (α^2v_S + v_n)}{α^2 v_S+v_n} + \frac{α^2v_S^2}{α^2 v_S+v_n}
        - 2 \frac{α^2v_S^2}{α^2 v_S+v_n} \\
    &=&& \frac{v_S ⋅ v_n}{α^2 v_S+v_n}
\end{aligned}
$$

#### 3. Comparing both

$$
\begin{aligned}
&& \E{(S-\hat{S}_1)^2} &> \E{(S-\hat{S}_2)^2} &⟺ \\
&⟺& \frac{v_S ⋅ v_n}{v_S + v_n} &> \frac{v_S ⋅ v_n}{α^2 v_S+v_n} &⟺\\
&⟺& α^2 v_S+v_n &> v_S + v_n &⟺\\
&⟺& |α| &> 1
\end{aligned}
$$

#### Results

$$
\begin{aligned}
    & \boxed{\E{(S-\hat{S}_1)^2} = \frac{v_S ⋅ v_n}{v_S + v_n}} \\
    & \boxed{\E{(S-\hat{S}_2)^2} = \frac{v_S ⋅ v_n}{α^2 v_S+v_n}} \\
    & \boxed{\E{(S-\hat{S}_1)^2} > \E{(S-\hat{S}_2)^2} ⟺ |α| > 1}
\end{aligned}
$$

### Question c) Find $\hat{S}_{MMSE}(\bar{X})$

Since the variables are linear and independent, their joint pdfs are also gaussian, so the MMSE
estimator is linear and can be expressed as

$$
\hat{S}_{MMSE}(\bar{X}) = \bar{w}_0 + w^T \bar{X}
$$

Substituting each parameter by its respective formula (as solved in the notes)

$$
\begin{aligned}
    \hat{S}_{MMSE}(\bar{X}) &= \E{S} - \bar{c}_{\bar{X},S}^T C_{\bar{X},\bar{X}}^{-1} \E{\bar{X}}
        + \left(C_{\bar{X}, \bar{X}}^{-1} c_{\bar{X}, S}\right)^T \bar{X} \\
    &= \E{S} + \bar{c}_{\bar{X},S}^T C_{\bar{X},\bar{X}}^{-1} \left(\bar{X} - \E{\bar{X}}\right) \\
    &= 0 + \begin{bmatrix}
            v_{SX_1} \\
            v_{SX_2}
        \end{bmatrix}^T \begin{bmatrix}
            v_{X_1} & v_{X_1X_2} \\
            v_{X_2 X_1} & v_{X_2}
        \end{bmatrix}^{-1} \begin{bmatrix}
            \left(X_1 - \E{X_1}\right) \\
            \left(X_2 - \E{X_2}\right)
        \end{bmatrix}
\end{aligned}
$$

Of those values, we're missing $v_{X_1X_2}$ and $v_{X_2X_1}$. Of course, they are equal.

$$
\begin{aligned}
    v_{X_1X_2} &= \E{X_1 X_2} - \E{X_1} \E{X_2} \\
    &= \E{(S + N_1)(αS + N_2)} - \E{S + N_1} \E{αS + N_2} \\
    &= \E{αS^2 + SN_2 + αSN_1 + N_1 N_2} - (0+0) (0+0) \\
    &= α\E{S^2} + \E{SN_2} + α\E{SN_1} + \E{N_1 N_2} \\
    &= α\Var{S} + \Cov(S,N_2) + α\Cov(S,N_1) + \Cov(N_1, N_2) \\
    &= αv_S
\end{aligned}
$$

Then, we can substitute

$$
\begin{aligned}
    \hat{S}_{MMSE} &= \begin{bmatrix}
            v_S \\
            α v_S
        \end{bmatrix}^T \begin{bmatrix}
            v_S + v_n & α v_S \\
            α v_S     & α^2 v_S + v_n
        \end{bmatrix}^{-1} \begin{bmatrix}
            X_1 \\
            X_2
        \end{bmatrix} \\

    &= \begin{bmatrix}
            v_S \\
            α v_S
        \end{bmatrix}^T
        \frac{1}{\left(v_S+v_n\right)\left(α^2v_S+v_n\right) - α^2v_S^2}
        \begin{bmatrix}
            α^2 v_S + v_n & -αv_S \\
            -αv_S         & v_S + v_n
        \end{bmatrix}
        \begin{bmatrix}
            X_1 \\
            X_2
        \end{bmatrix} \\
    &= \frac{1}{α^2v_S^2 + v_S v_n + α^2 v_S v_n + v_n^2 - α^2 v_S^2}
        \begin{bmatrix}
            v_S (α^2 v_S + v_n) - α^2 v_S^2 \\
            - α v_S^2 + α v_S (v_S + v_n)
        \end{bmatrix}
        \begin{bmatrix}
            X_1 \\
            X_2
        \end{bmatrix} \\
    &= \frac{1}{(1+α^2) v_S v_n + v_n^2} \left(v_S v_n X_1 + αv_s v_n X_2\right) \\
    &= \boxed{\frac{1}{1+α^2 +\frac{v_n}{v_S}} X_1 + \frac{α X_2}{1+α^2 +\frac{v_n}{v_S}} X_2}
\end{aligned}
$$

## Exercise 6 (1.2; 1.3; 1.4; 1.7)

### Question a) Find $p_{X|S}(x|s)$

Let's use the properties of variable transformations. First, we'll need to find the expression of
$R$ in terms of $X$ and $S$.

$$
R = X ⋅ S
$$

Now, we'll apply the formula for the PDF of a transformed variable:

$$
\begin{aligned}
    p_{X|S}(x|s) &= p_{R|S} (r = x⋅s) \left|\frac{∂}{∂x} x⋅s\right| \\
    &= \exp (-x⋅s)⋅|s|, \quad x⋅s > 0 \\
    &= \boxed{s⋅\exp (-x⋅s), \quad x > 0}
\end{aligned}
$$

### Question b) Find $\ML{S}$

$$
\ML{S} = \arg \max_s p_{X|S}(x|s)
$$

To find this,

$$
\begin{aligned}
    \frac{∂}{∂s} p_{X|S}(x|s) \Bigg|_{S=\ML{S}} &= 0 \\
    \exp(-x⋅\ML{s}) + -x\ML{s} ⋅ \exp(-x⋅\ML{s}) &= 0 \\
    (1 - x\ML{s}) \exp(-x⋅\ML{s}) &= 0 \\
    1 - x\ML{s} &= 0 \\
    \boxed{\ML{s} = \frac{1}{x}}
\end{aligned}
$$

### Question c) Find $p_{S,X}(s,x)$ and $p_{S|X}(s|x)$

$$
\begin{aligned}
    p_{S,X}(s,x) &= p_{X|S}(x|s) p_S(s) \\
    &= s \exp(-x⋅s) \exp(-s), \quad x > 0\\
    &= \boxed{s \exp(-s(1+x)), \quad x > 0}
\end{aligned}
$$

Now, for the *a posteriori* distribution:

$$
p_{S|X}(s|x) = \frac{p_{S,X}(s,x)}{p_X(x)}
$$

First, we'll have to find $p_X(x)$. It can be found as the marginal of $p_{S,X}(s,x)$:

$$
\begin{aligned}
    p_X(x) &= ∫_{(S)} p_{S,X}(s,x) ds \\
    &= ∫_0^∞ s \exp(-s(1+x)) ds, \quad x>0\\
    &= \left.\frac{-s}{x+1} \exp(-s(x+1))\right|_{s=0}^∞ + \frac{1}{x+1} ∫_0^∞ \exp(-s(x+1)) ds \\
    &= \frac{-1}{(x+1)^2}\exp(-s(x+1))\Big|_{s=0}^∞ \\
    &= \frac{1}{(x+1)^2}, \quad x>0
\end{aligned}
$$

And then,

$$
\boxed{p_{S|X}(s|x) = (x+1)^2 s \exp(-s(1+x)), \quad x>0}
$$

### Question d) Find $\MAP{S}(x)$

$$
\MAP{S}(x) = \arg \max_s p_{S|X}(s|x)
$$

To find this,

$$
\begin{aligned}
    \frac{∂}{∂s} p_{S|X} (s|x) \Bigg|_{s=\MAP{s}} &= 0 \\
    (x+1)^2 \Big(\exp(-s(x+1)) - s(x+1) \exp(-s(x+1))\Big) &= 0 \\
    1 - s(x+1) &= 0 \\
    \boxed{\MAP{s} = \frac{1}{x+1}}
\end{aligned}
$$

### Question e) Find $\MSE{S}(x)$

$$
\begin{aligned}
    \MSE{s}(x) &=&& \E{S|X=x} \\
    &=&& ∫_{(S)} s p_{S|X}(s|x) ds \\
    &=&& ∫_0^∞ s (x+1)^2 s \exp(-s(1+x)) ds, \quad x>0 \\
    &=&& (x+1)^2 \left.\frac{-s^2}{(x+1)} \exp(-s(x+1))\right|_{s=0}^∞ \\
        &&& + \frac{1}{x+1} ∫_0^∞ 2s \exp(-s(x+1)) ds \\
    &=&& 0 + 2 (x+1) \left. \frac{-s}{x+1} \exp(-s(x+1)) \right|_{s=0}^∞ \\
        &&& + \frac{1}{x+1} ∫_0^∞ \exp(-s(x+1)) ds \\
    &=&& - \frac{2}{x+1} \exp(-s(x+1))\Big|_{s=0}^∞ \\
    &=&& \boxed{\frac{2}{x+1}}
\end{aligned}
$$

### Question f) Find the bias of $\MAP{S}$ and $\MSE{S}$

#### 1. Bias of $\MAP{S}$

$$
\begin{aligned}
    \E{S-\MAP{S}} &= \E{S -\frac{1}{x+1}} \\
    &= \E{S} - \E{\frac{1}{x+1}} \\
\end{aligned}
$$

We'll find each of those expected values separately.

$$
\begin{aligned}
    \E{S} &= ∫_{(S)} s p_S(s) ds \\
    &= ∫_0^∞ s \exp(-s) ds \\
    &= \left. -s \exp(-s) \right|_{s=0}^∞ + ∫_0^∞ \exp(-s) ds \\
    &= 0 - \exp(-s) \Big|_{s=0}^∞ \\
    &= +1
\end{aligned}
$$

$$
\begin{aligned}
    \E{\frac{1}{x+1}} &= ∫_{(X)} \frac{1}{x+1} p_X(x) dx \\
    &= ∫_0^∞ \frac{1}{(x+1)^3} dx \\
    &= \frac{(x+1)^{-2}}{-2} \Big|_{x=0}^∞ \\
    &= \frac{1}{2}
\end{aligned}
$$

Substituting these values back into the original equation:

$$
\begin{aligned}
    \E{S-\MAP{S}} &= 1 - \frac{1}{2} \\
    &= \boxed{\frac{1}{2}}
\end{aligned}
$$

#### 2. Bias of $\MSE{S}$

$$
\begin{aligned}
    \E{S-\MSE{S}} &= \E{S - \frac{2}{x+1}} \\
    &= \E{S} - 2\E{\frac{1}{x+1}} \\
    &= 1 - 2 \frac{1}{2} \\
    &= \boxed{0}
\end{aligned}
$$

## Exercise 12 (1.2; 1.3)

### Question a) Find the marginal of $X$, $p_X(x)$ specifying $α$

To find $α$, we'll use the fact that the integral of the whole joint pdf must be equal to 1.

$$
\begin{aligned}
    ∫_{(X)} ∫_{(S)} p_{S,X}(s,x) ds dx &= 1 \\
    ∫_{-1}^1 ∫_0^{|x|} α ds dx &= 1 \\
    α ∫_{-1}^1 |x| dx &= 1 \\
    α ⋅ 2⋅∫_0^1 x dx &= 1 \\
    α ⋅ 2 \left. \frac{x^2}{2} \right|_0^1 &= 1 \\
    \boxed{α = 1}
\end{aligned}
$$

With that, we can go on to find $p_X(x)$

$$
\begin{aligned}
    p_X(x) &= ∫_{(S)} p_{S,X}(s,x) ds \\
    &= ∫_0^{|x|} 1 ds \\
    &= \boxed{|x|, \quad -1 < x < 1}
\end{aligned}
$$

### Question b) Find $\MSE{S}$ and $\MAD{S}$

#### Finding $\MSE{S}$

We'll first find the conditional pdf of $S$ given $X$.

$$
p_{S|X}(s|x) = \frac{p_{S,X}(s,x)}{p_X(x)} = \frac{1}{|x|}, \quad 0 < s < |x|
$$

Then, the estimator is the expected value of this conditional pdf.

$$
\begin{aligned}
    \MSE{S} = \E{S|X} &= ∫_{(S)} s p_{S|X}(s|x) ds \\
    &= ∫_0^{|x|} s \frac{1}{|x|} ds \\
    &= \frac{1}{|x|} \left. \frac{s^2}{2} \right|_0^{|x|} \\
    &= \frac{1}{|x|} \frac{x^2}{2} \\
    \MSE{S} &= \boxed{\frac{|x|}{2}}
\end{aligned}
$$

#### Finding $\MAD{S}$

This is the median of the conditional pdf of $S$ given $X$, which means:

$$
\begin{aligned}
    ∫_0^{\MAD{S}} \frac{1}{|x|} ds &= \frac{1}{2} \\
    \frac{1}{|x|} \MAD{S} &= \frac{1}{2} \\
    \left.\frac{s}{|x|}\right|_{s=0}^{\MAD{S}} &= \frac{1}{2} \\
    \frac{\MAD{S}}{|x|} &= \frac{1}{2} \\
    \MAD{S} &= \boxed{\frac{|x|}{2}}
\end{aligned}
$$

#### Note

This all could have been easily resolved graphically by noticing that the conditional pdf of $S$
given $X$ is a uniform distribution, so the mean and the median are the same.

### Question c) Find $\est{S}{q,MSE}=w_1X^2$ and $\est{S}{q,MAD}=w_2X^2$

#### Finding $\est{S}{q,MSE}$

Let's find the $MSE$ expected cost for this estimator

$$
\begin{aligned}
    \E{C(S, \hat{S})} &= ∫_{(X)} ∫_{(S)} (S - \hat{S})^2 p_{S,X}(s,x) ds dx \\
    &= ∫_{-1}^1 ∫_0^{|x|} (S - w_1 x^2)^2 ds dx \\
\end{aligned}
$$

To find the appropriate value of $w_1$, we'll minize the cost function

$$
\begin{aligned}
    \frac{∂}{∂w_1} ∫_{-1}^1 ∫_0^{|x|} (s - w_1 x^2)^2 ds dx &= 0 \\
    ∫_{-1}^1 ∫_0^{|x|} -2 (s - w_1 x^2) x^2 ds dx &= 0 \\
    ∫_{-1}^1 x^2 ⋅ \left(\frac{x^2}{2} - w_1 |x| ⋅x^2\right) dx &= 0 \\
    ∫_{-1}^1 \frac{x^4}{2} - w_1 |x| x^4 dx &= 0 \\
    2 ⋅ ∫_0^1 \frac{x^4}{2} - w_1 x^5 dx &= 0 \\
    2 ⋅ \left(\frac{1^5}{10} - \frac{w_1 ⋅ 1^6}{6}\right) &= 0 \\
    w_1 &= \frac{3}{5}
\end{aligned}
$$

Therefore, $\boxed{\est{S}{q,MSE} = \frac{3}{5} X^2}$

#### Finding $\est{S}{q,MAD}$

$$
C(S,\hat{S}) = |S - w_2 x^2| \\
$$

Now minimizing this cost function

$$
\begin{aligned}
    \frac{∂}{∂w_2} \E{C(S,\hat{S})} &= 0 \\
    \frac{∂}{∂w_2} ∫_{-1}^1 ∫_0^{|x|} |s - w_2 x^2| ds dx &= 0 \\
    2 ⋅ ∫_0^1 ∫_0^x \frac{s-w_2x^2}{|s-w_2x^2|} x^2 ds dx &= 0 \\
    ∫_0^1 x^2 ∫_0^x \sgn(s-w_2x^2) ds dx &= 0 \\
    \left[\sgn(s-w_2x^2) = \begin{cases}
        1 & s > w_2x^2 \\
        -1 & s < w_2x^2
    \end{cases}\right] &\Bigg\Updownarrow \\

    ∫_0^1 x^2 \left( ∫_0^{\min(x, w_2x^2)} -1 ds + ∫_{\min(x, w_2x^2)}^x 1 ds\right) dx &= 0\\

    \left[\min(x, w_2x^2) = \begin{cases}
        x & \text{if } x < w_2x^2 ⟺ x < \frac{1}{w_2} \\
        w_2x^2 & \text{if } x > w_2x^2 ⟺ x > \frac{1}{w_2} \\
    \end{cases}\right] & \Bigg\Updownarrow \\

    ∫_0^1 x^2 \left(\begin{cases}
        -∫_0^{w_2x^2} ds + ∫_{w_2x^2}^x ds & \text{if }\; x < \frac{1}{w_2} \\
        -∫_0^x ds & \text{if }\; x > \frac{1}{w_2} \\
    \end{cases}\right) dx &= 0 \\

    ∫_0^{\min(1, \frac{1}{w_2})} x^2 \left(-∫_0^{w_2x^2} ds + ∫_{w_2x^2}^x ds\right) dx
        - ∫_{\min(1, \frac{1}{w_2})}^1 x^2 ∫_0^x ds dx &= 0 \\

    \begin{cases}
        ∫_0^1 x^2 \left(-∫_0^{w_2x^2} ds + ∫_{w_2x^2}^x ds\right) dx
            & \text{if } \frac1{w_2} < 1 \\
        ∫_0^{\frac1{w_2}} x^2 \left(-∫_0^{w_2x^2} ds + ∫_{w_2x^2}^x ds\right) dx
        - ∫_{\frac1{w_2}}^1 x^2 ∫_0^x ds dx
            & \text{if } \frac1{w_2} > 1 \\
    \end{cases} &= 0 \\
\end{aligned}
$$

* If $w_2 < 1$:

    $$
    \begin{aligned}
        ∫_0^1 x^2 ( -w_2 x^2 + x - w_2 x^2) dx &= 0 \\
        ∫_0^1 x^3 - 2w_2 x^4 dx &= 0 \\
        \left. \frac{x^4}{4} - \frac{2w_2 x^5}{5} \right|_0^1 &= 0 \\
        \frac{1}{4} - \frac{2w_2}{5} &= 0 \\
        w_2 &= \frac{5}{8}
    \end{aligned}
    $$

* If $w_2 > 1$:

    $$
    \begin{aligned}
        ∫_0^{\frac1{w_2}} x^2 \left(-∫_0^{w_2x^2} ds + ∫_{w_2x^2}^x ds\right) dx
            - ∫_{\frac1{w_2}}^1 x^2 ∫_0^x ds dx &= 0 \\
        ∫_0^{\frac1{w_2}} x^2 \left(-2w_2 x^2 + x\right) dx - ∫_{\frac1{w_2}}^1 x^2⋅x dx &= 0 \\
        ∫_0^{\frac1{w_2}} -2w_2 x^4 + x^3 dx - ∫_{\frac1{w_2}}^1 x^3 dx &= 0 \\
        \left. -2w_2 \frac{x^5}{5} + \frac{x^4}{4} \right|_0^{\frac1{w_2}}
            - \left. \frac{x^4}{4} \right|_{\frac1{w_2}}^1 &= 0 \\
        -2w_2 \frac{1}{5w_2^5} + \frac{1}{4w_2^4} - \frac{1}{4} + \frac{1}{4w_2^4} &= 0 \\
        \frac{1}{10w_2} &= \frac{1}{4} \\
        w_2 &= \frac{2}{5}
    \end{aligned}
    $$

    Which is a contradiction.

Therefore, we can conclude that

$$
\boxed{\est{S}{q,MAD} = \frac{5}{8} X^2}
$$

## Exercise 17 (1.4; 1.7)

### Question a) Find $\ML{B}$

From the given pdf (which can be interpreted as conditional on a known $b$)

$$
p_{X|B}(x|b) = \frac{1}{bx^2} \exp\left(\frac{-1}{bx}\right), \quad x > 0
$$

We can get

$$
p(\set{x^{(k)}}|b) = ∏_{k=1}^k \frac{1}{b(x^{(k)})^2} \exp\left(\frac{-1}{b(x^{(k)})}\right)
$$

To simplify the operations with the exponential, we'll tweak the formula for the ML estimator to
include a logarithm. The $\argmax$ will be the same.

$$
\begin{aligned}
    \ML{B} &= \argmax_b p\left(\set{x^{(k)}} | b\right) \\
    & = \argmax_b \left(\ln p\left(\set{x^{(k)}} | b\right)\right)
\end{aligned}
$$

First, we'll simplify the term inside $\argmax$

$$
\begin{aligned}
    \ln p\left(\set{x^{(k)}}|b\right)
    &= \ln \left(∏_{k=1}^K \frac{1}{b⋅(x^{(k)})^2}\right) \exp\left(\frac{-1}{b⋅x^{(k)}}\right) \\
    &= ∑_{k=1}^K \ln\left(\frac{1}{b⋅(x^{(k)})^2}\right) - \frac{1}{b} ∑_{k=1}^K \frac{1}{x^{(k)}}\\
    &= -∑_{k=1}^K \ln\left(b⋅(x^{(k)})^2\right) - \frac{1}{b} ∑_{k=1}^K \frac{1}{x^{(k)}} \\
    &= -k \ln b - 2 ∑_{k=1}^K \ln x^{(k)} - \frac{1}{b} ∑_{k=1}^K \frac{1}{x^{(k)}}
\end{aligned}
$$

And then, we'll take the partial derivative to maximize the function

$$
\begin{aligned}
    \frac{∂}{∂b} \ln p\left(\set{x^{(k)}}|b\right) \Bigg|_{b=\ML{B}} &= 0 \\
    -\frac{K}{\ML{b}} + \frac{1}{\ML{b}^2} ∑_{k=1}^K \frac{1}{x^{(k)}} &= 0 \\
    \frac{1}{\ML{b}} ∑_{k=1}^K \frac{1}{x^{(k)}} &= K \\
    \boxed{\ML{b} = \frac{1}{K} ∑_{k=1}^K \frac{1}{x^{(k)}}}
\end{aligned}
$$

### Question b) Verify that $Y=1/X$ has unilateral exponential pdf and find its mean

#### Prove that $Y$ has a unilateral exponential pdf

We'll perform a variable transformation to find the pdf of $Y$.

$$
p_Y(y) = p_X(x(y)) ⋅ \left|\frac{∂}{∂y} x(y)\right|
$$

We'll need the expression of $X$ in terms of $Y$.

$$
X = \frac{1}{Y}
$$

Then, we can find the pdf of $Y$

$$
\begin{aligned}
    p_Y(y) &= p_X\left(\frac{1}{y}\right) ⋅ \left|\frac{∂}{∂y} \frac{1}{y}\right|
        , \quad \frac{1}{y}>0 \\
    &= \frac{y^2}{b} \exp\left(-\frac{y}{b}\right) ⋅ \left|\frac{-1}{y^2}\right| \\
    &= \boxed{\frac{1}{b} \exp\left(-\frac{1}{b}\right), \quad y > 0}
\end{aligned}
$$

And indeed, it is a unilateral exponential pdf.

#### Find the mean

Now, we'll find the mean of $Y$.

$$
\begin{aligned}
    \E{Y} &= ∫_{(Y)} y p_Y(y) dy \\
    &= ∫_0^∞ y \frac{1}{b} \exp\left(-\frac{1}{b}\right) dy \\
    &= \frac{1}{b} \left. -yb \exp\left(-\frac{1}{b}\right) \right|_{y=0}^∞
        + b ∫_0^∞ \exp\left(-\frac{1}{b}\right) dy \\
    &= 0 - b \exp\left(-\frac{1}{b}\right) \Big|_{y=0}^∞ \\
    &= \boxed{b}
\end{aligned}
$$

### Question c) Is $\ML{B}$ biased?

Well, let's find the bias

$$
\begin{aligned}
    \E{\ML{B} - b} &= \E{\ML{B}} - b \\
    &= \E{\frac{1}{K} ∑_{k=1}^K \frac{1}{x^{(k)}}} - b \\
    &= \frac{1}{K} ∑_{k=1}^K \E{\frac{1}{x^{(k)}}} - b \\
    &= \frac{1}{K} ⋅ K ⋅ \E{y^{(k)}} - b \\
    &= b - b \\
    &= \boxed{0}
\end{aligned}
$$

Therefore, the ML estimator $\ML{B}$ is **unbiased**.

## Modern Theory of Detection and Estimation

# Analytic Estimation Problems - Answers

*Academic year 2024-2025*  

$$
\global\def\E#1{\mathbb E \left\{#1\right\}}
\global\def\Var#1{\mathrm{Var} \left\{#1\right\}}
\global\def\Cov{\mathrm{Cov}}
$$

---

## Exercise 4 (1.5)

### Question a) Find $\hat{S}_1$ and $\hat{S}_2$

#### 1. Finding $\hat{S}_1$

We are tasked with finding an expression for the estimator of $S$ given $X_1$, $\hat{S}_1$.

This can be found using the known formula:

$$
\hat{S}_1 = m_S + \frac{v_{SX_1}}{v_{X_1}} (X_1 - m_{X_1})
$$

We just need to find the value of the parameters.

$$
\begin{aligned}
    m_S &= 0 \phantom{\underset{0}{\normal0}}\\
    m_{X_1} &= \E{S + N_1} \\
        &= \E{S} + \E{N_1} \\
        &= 0 \phantom{\underset{0}{\normal0}}\\
    v_{SX_1} &= \E{SX_1} - \E{S}\E{X_1} \\
        &= \E{S (S+N_1)} - 0 \\
        &= \E{S^2 + SN_1} \\
        &= \E{S^2} + \E{SN_1} \\
        &= \Var{S} + \E{S}^2 + \Cov(S, N_1) + \E{S}\E{N_1} \\
        &= v_S \phantom{\underset{0}{\normal0}}\\
    v_{X_1} &= \E{X_1^2} - \E{X_1}^2 \\
        &= \E{(S + N_1)^2} - \E{S + N_1}^2 \\
        &= \E{S^2 + 2SN_1 + N_1^2} - \left(\E{S} + \E{N_1}\right)^2 \\
        &= \E{S^2} + 2\E{SN_1} + \E{N_1^2} - 0 \\
        &= \Var{S} + \E{S}^2 +  \Var{N_1} + \E{N_1}^2 + 2\E{S}\E{N_1} \\
        &= v_S + v_n
\end{aligned}
$$

Substituting these values into the formula for $\hat{S}_1$:

$$
\begin{aligned}
    \hat{S}_1 &= 0 + \frac{v_S}{v_S + v_n} (X_1 - 0) \\
    &\boxed{= \frac{v_S}{v_S + v_n} X_1}
\end{aligned}
$$

#### 2. Finding $\hat{S}_2$

This will be similar to the previous part, but using $X_2$ instead of $X_1$.

$$
\hat{S}_2 = m_S + \frac{v_{SX_2}}{v_{X_2}} (X_2 - m_{X_2})
$$

Next, to find the values of the parameters

$$
\begin{aligned}
    m_S &= 0 \phantom{\underset{0}{\normal0}}\\
    m_{X_2} &= \E{S + N_2} \\
        &= \E{S} + \E{N_2} \\
        &= 0 \phantom{\underset{0}{\normal0}}\\
    v_{SX_2} &= \E{SX_2} - \E{S}\E{X_2} \\
        &= \E{S (αS+N_2)} - \E{S} \E{αS+N_2} \\
        &= \E{αS^2 + SN_2} - 0\\
        &= α\E{S^2} + \E{SN_2} \\
        &= α\left(\Var{S} + \E{S}^2\right) + \Cov(S, N_2) + \E{S}\E{N_2} \\
        &= αv_S \phantom{\underset{0}{\normal0}}\\
    v_{X_2} &= \E{X_2^2} - \E{X_2}^2 \\
        &= \E{(αS + N_2)^2} - \E{αS + N_2}^2 \\
        &= \E{α^2S^2 + 2αSN_2 + N_2^2} - \left(\E{αS} + \E{N_2}\right)^2 \\
        &= α^2\E{S^2} + 2α\E{SN_2} + \E{N_2^2} - 0 \\
        &= α^2\left(\Var{S} + \E{S}^2\right) + \Var{N_2} + \E{N_2}^2 + 2\E{S}\E{N_2} \\
        &= α^2 v_S + v_n
\end{aligned}
$$

With these values, we can substitute them into the formula for $\hat{S}_2$:

$$
\begin{aligned}
    \hat{S}_2 &= 0 + \frac{αv_S}{α^2 v_S + v_n} (X_2 - 0) \\
    &\boxed{= \frac{αv_S}{α^2 v_S + v_n} X_2}
\end{aligned}
$$

### Question b) Find MSE of each estimator and compare for different values of α

The mean square error is defined as:

$$
\begin{aligned}
    \E{(S-\hat{S})^2}
\end{aligned}
$$

#### 1. Finding $\E{(S-\hat{S}_1)^2}$

$$
\begin{aligned}
    \E{(S-\hat{S}_1)^2} &= \E{\left(S - \frac{v_S}{v_S + v_n} X_1\right)^2} \\
    &= \E{S^2} - 2\frac{v_S}{v_S+v_n} \E{S X_1} + \frac{v_S^2}{(v_S+v_n)^2} \E{X_1^2} \\
    &= \Var{S} + \frac{v_S^2}{(v_S+v_n)^2} \E{(S + N_1)^2}
        + 2 \frac{v_S}{v_S+v_n} \E{S(S+N_1)} \\
    &= v_S + \frac{v_S^2}{(v_S+v_n)^2} \left(\E{S^2} + \E{N_1^2} + 2\E{SN_1}\right)
        - 2 \frac{v_S}{v_S+v_n} \left(\E{S^2} + \E{S N_1}\right) \\
    &= v_S + \frac{v_S^2}{(v_S+v_n)^2} (\Var{S} + \Var{N_1})
        - 2 \frac{v_S}{v_S+v_n} (\Var{S}) \\
    &= v_S + \frac{v_S^2}{v_S+v_n} - 2 \frac{v_S^2}{v_S+v_n} \\
    &= \frac{v_S (v_S + v_n)}{v_S + v_n} - \frac{v_S^2}{v_S+v_n} \\
    &= \frac{v_S ⋅ v_n}{v_S + v_n}
\end{aligned}
$$

#### 2. Finding $\E{(S-\hat{S}_2)^2}$

$$
\begin{aligned}
    \E{(S-\hat{S}_2)^2} &=&& \E{\left(S - \frac{αv_S}{α^2 v_S + v_n} X_2\right)^2} \\
    &=&& \E{S^2} + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2} \E{X_2^2}
        - 2 \frac{αv_S}{α^2 v_S+v_n} \E{SX_2} \\
    &=&& \Var{S} + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2} \E{(αS + N_2)^2}
        - \frac{2 αv_S}{α^2 v_S+v_n} \E{S(αS+N_2)} \\
    &=&& v_S + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2} \left(α^2\E{S^2} + \E{N_2^2} +2α\E{SN_2}\right) \\
      &&& - \frac{2 αv_S}{α^2 v_S+v_n} \left(α\E{S^2} + \E{S N_2}\right) \\
    &=&& v_S + \frac{α^2 v_S^2}{(α^2 v_S+v_n)^2}\left(α^2v_S + v_n\right)
        - \frac{2 αv_S}{α^2 v_S+v_n} αv_S \\
    &=&& \frac{v_S (α^2v_S + v_n)}{α^2 v_S+v_n} + \frac{α^2v_S^2}{α^2 v_S+v_n}
        - 2 \frac{α^2v_S^2}{α^2 v_S+v_n} \\
    &=&& \frac{v_S ⋅ v_n}{α^2 v_S+v_n}
\end{aligned}
$$

#### 3. Comparing both

$$
\begin{aligned}
&& \E{(S-\hat{S}_1)^2} &> \E{(S-\hat{S}_2)^2} &⟺ \\
&⟺& \frac{v_S ⋅ v_n}{v_S + v_n} &> \frac{v_S ⋅ v_n}{α^2 v_S+v_n} &⟺\\
&⟺& α^2 v_S+v_n &> v_S + v_n &⟺\\
&⟺& |α| &> 1
\end{aligned}
$$

#### Results

$$
\begin{aligned}
    & \boxed{\E{(S-\hat{S}_1)^2} = \frac{v_S ⋅ v_n}{v_S + v_n}} \\
    & \boxed{\E{(S-\hat{S}_2)^2} = \frac{v_S ⋅ v_n}{α^2 v_S+v_n}} \\
    & \boxed{\E{(S-\hat{S}_1)^2} > \E{(S-\hat{S}_2)^2} ⟺ |α| > 1}
\end{aligned}
$$

### Question c) Find $\hat{S}_{MMSE}(\bar{X})$

Since the variables are linear and independent, their joint pdfs are also gaussian, so the MMSE
estimator is linear and can be expressed as

$$
\hat{S}_{MMSE}(\bar{X}) = \bar{w}_0 + w^T \bar{X}
$$

Substituting each parameter by its respective formula (as solved in the notes)

$$
\begin{aligned}
    \hat{S}_{MMSE}(\bar{X}) &= \E{S} - \bar{c}_{\bar{X},S}^T C_{\bar{X},\bar{X}}^{-1} \E{\bar{X}}
        + \left(C_{\bar{X}, \bar{X}}^{-1} c_{\bar{X}, S}\right)^T \bar{X} \\
    &= \E{S} + \bar{c}_{\bar{X},S}^T C_{\bar{X},\bar{X}}^{-1} \left(\bar{X} - \E{\bar{X}}\right) \\
    &= 0 + \begin{bmatrix}
            v_{SX_1} \\
            v_{SX_2}
        \end{bmatrix}^T \begin{bmatrix}
            v_{X_1} & v_{X_1X_2} \\
            v_{X_2 X_1} & v_{X_2}
        \end{bmatrix}^{-1} \begin{bmatrix}
            \left(X_1 - \E{X_1}\right) \\
            \left(X_2 - \E{X_2}\right)
        \end{bmatrix}
\end{aligned}
$$

Of those values, we're missing $v_{X_1X_2}$ and $v_{X_2X_1}$. Of course, they are equal.

$$
\begin{aligned}
    v_{X_1X_2} &= \E{X_1 X_2} - \E{X_1} \E{X_2} \\
    &= \E{(S + N_1)(αS + N_2)} - \E{S + N_1} \E{αS + N_2} \\
    &= \E{αS^2 + SN_2 + αSN_1 + N_1 N_2} - (0+0) (0+0) \\
    &= α\E{S^2} + \E{SN_2} + α\E{SN_1} + \E{N_1 N_2} \\
    &= α\Var{S} + \Cov(S,N_2) + α\Cov(S,N_1) + \Cov(N_1, N_2) \\
    &= αv_S
\end{aligned}
$$

Then, we can substitute

$$
\begin{aligned}
    \hat{S}_{MMSE} &= \begin{bmatrix}
            v_S \\
            α v_S
        \end{bmatrix}^T \begin{bmatrix}
            v_S + v_n & α v_S \\
            α v_S     & α^2 v_S + v_n
        \end{bmatrix}^{-1} \begin{bmatrix}
            X_1 \\
            X_2
        \end{bmatrix} \\

    &= \begin{bmatrix}
            v_S \\
            α v_S
        \end{bmatrix}^T
        \frac{1}{\left(v_S+v_n\right)\left(α^2v_S+v_n\right) - α^2v_S^2}
        \begin{bmatrix}
            α^2 v_S + v_n & -αv_S \\
            -αv_S         & v_S + v_n
        \end{bmatrix}
        \begin{bmatrix}
            X_1 \\
            X_2
        \end{bmatrix} \\
    &= \frac{1}{α^2v_S^2 + v_S v_n + α^2 v_S v_n + v_n^2 - α^2 v_S^2}
        \begin{bmatrix}
            v_S (α^2 v_S + v_n) - α^2 v_S^2 \\
            - α v_S^2 + α v_S (v_S + v_n)
        \end{bmatrix}
        \begin{bmatrix}
            X_1 \\
            X_2
        \end{bmatrix} \\
    &= \frac{1}{(1+α^2) v_S v_n + v_n^2} \left(v_S v_n X_1 + αv_s v_n X_2\right) \\
    &= \boxed{\frac{1}{1+α^2 +\frac{v_n}{v_S}} X_1 + \frac{α X_2}{1+α^2 +\frac{v_n}{v_S}} X_2}
\end{aligned}
$$
